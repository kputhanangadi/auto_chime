{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 02:06:10.395521: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-20 02:06:10.525957: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-20 02:06:10.526972: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 02:06:11.959312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception V3 model for Keras\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    " \n",
    "# To detect objects, we will use a pretrained neural network that has been \n",
    "# trained on the COCO data set. You can read more about this data set here: \n",
    "#   https://content.alegion.com/datasets/coco-ms-coco-dataset\n",
    "# COCO labels are here: https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt\n",
    "LABEL_PERSON = 1\n",
    "LABEL_CAR = 3\n",
    "LABEL_BUS = 6\n",
    "LABEL_TRUCK = 8\n",
    "LABEL_TRAFFIC_LIGHT = 10\n",
    "LABEL_STOP_SIGN = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import CENTER\n",
    "\n",
    "\n",
    "def accept_box(boxes, box_index, tolerance):\n",
    "  \"\"\"\n",
    "  Eliminate duplicate bounding boxes.\n",
    "  \"\"\"\n",
    "  box = boxes[box_index]\n",
    " \n",
    "  for idx in range(box_index):\n",
    "    other_box = boxes[idx]\n",
    "    if abs(CENTER(other_box, \"x\") - CENTER(box, \"x\")) < tolerance and abs(CENTER(other_box, \"y\") - CENTER(box, \"y\")) < tolerance:\n",
    "      return False\n",
    " \n",
    "  return True\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(pattern):\n",
    "  \"\"\"\n",
    "  Create a list of all the images in a directory\n",
    "     \n",
    "  :param:pattern str The pattern of the filenames\n",
    "  :return: A list of the files that match the specified pattern \n",
    "  \"\"\"\n",
    "  files = []\n",
    " \n",
    "  # For each file that matches the specified pattern\n",
    "  for file_name in glob.iglob(pattern, recursive=True):\n",
    " \n",
    "    # Add the image file to the list of files\n",
    "    files.append(file_name)\n",
    " \n",
    "  # Return the complete file list\n",
    "  return files\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  \"\"\"\n",
    "  Download a pretrained object detection model, and save it to your hard drive.\n",
    "  :param:str Name of the pretrained object detection model\n",
    "  \"\"\"\n",
    "  url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + model_name + '.tar.gz'\n",
    "     \n",
    "  # Download a file from a URL that is not already in the cache\n",
    "  model_dir = tf.keras.utils.get_file(fname=model_name, untar=True, origin=url)\n",
    " \n",
    "  print(\"Model path: \", str(model_dir))\n",
    "   \n",
    "  model_dir = str(model_dir) + \"/saved_model\"\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    " \n",
    "  return model\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rgb_images(pattern, shape=None):\n",
    "  \"\"\"\n",
    "  Loads the images in RGB format.\n",
    "     \n",
    "  :param:pattern str The pattern of the filenames\n",
    "  :param:shape Image dimensions (width, height)\n",
    "  \"\"\"\n",
    "  # Get a list of all the image files in a directory\n",
    "  files = get_files(pattern)\n",
    " \n",
    "  # For each image in the directory, convert it from BGR format to RGB format\n",
    "  images = [cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2RGB) for file in files]\n",
    " \n",
    "  # Resize the image if the desired shape is provided\n",
    "  if shape:\n",
    "    return [cv2.resize(img, shape) for img in images]\n",
    "  else:\n",
    "    return images\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ssd_coco():\n",
    "  \"\"\"\n",
    "  Load the neural network that has the SSD architecture, trained on the COCO\n",
    "  data set.\n",
    "  \"\"\"\n",
    "  return load_model(\"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_annotated(img_rgb, file_name, output, model_traffic_lights=None):\n",
    "  \"\"\"\n",
    "  Annotate the image with the object types, and generate cropped images of\n",
    "  traffic lights.\n",
    "  \"\"\"\n",
    "  # Create annotated image file \n",
    "  output_file = file_name.replace('.jpg', '_test.jpg')\n",
    "     \n",
    "  # For each bounding box that was detected  \n",
    "  for idx in range(len(output['boxes'])):\n",
    " \n",
    "    # Extract the type of the object that was detected\n",
    "    obj_class = output[\"detection_classes\"][idx]\n",
    "     \n",
    "    # How confident the object detection model is on the object's type\n",
    "    score = int(output[\"detection_scores\"][idx] * 100)\n",
    "         \n",
    "    # Extract the bounding box\n",
    "    box = output[\"boxes\"][idx]\n",
    " \n",
    "    color = None\n",
    "    label_text = \"\"\n",
    " \n",
    "    if obj_class == LABEL_PERSON:\n",
    "      color = (0, 255, 255)\n",
    "      label_text = \"Person \" + str(score)\n",
    "    if obj_class == LABEL_CAR:\n",
    "      color = (255, 255, 0)\n",
    "      label_text = \"Car \" + str(score)\n",
    "    if obj_class == LABEL_BUS:\n",
    "      color = (255, 255, 0)\n",
    "      label_text = \"Bus \" + str(score)\n",
    "    if obj_class == LABEL_TRUCK:\n",
    "      color = (255, 255, 0)\n",
    "      label_text = \"Truck \" + str(score)\n",
    "    if obj_class == LABEL_STOP_SIGN:\n",
    "      color = (128, 0, 0)\n",
    "      label_text = \"Stop Sign \" + str(score)\n",
    "    if obj_class == LABEL_TRAFFIC_LIGHT:\n",
    "      color = (255, 255, 255)\n",
    "      label_text = \"Traffic Light \" + str(score)\n",
    "             \n",
    "      if model_traffic_lights:\n",
    "        # Annotate the image and save it\n",
    "        img_traffic_light = img_rgb[box[\"y\"]:box[\"y2\"], box[\"x\"]:box[\"x2\"]]\n",
    "        img_inception = cv2.resize(img_traffic_light, (299, 299))\n",
    "         \n",
    "        # Uncomment this if you want to save a cropped image of the traffic light\n",
    "        #cv2.imwrite(output_file.replace('.jpg', '_crop.jpg'), cv2.cvtColor(img_inception, cv2.COLOR_RGB2BGR))\n",
    "        img_inception = np.array([preprocess_input(img_inception)])\n",
    " \n",
    "        prediction = model_traffic_lights.predict(img_inception)\n",
    "        label = np.argmax(prediction)\n",
    "        score_light = str(int(np.max(prediction) * 100))\n",
    "        if label == 0:\n",
    "          label_text = \"Green \" + score_light\n",
    "        elif label == 1:\n",
    "          label_text = \"Yellow \" + score_light\n",
    "        elif label == 2:\n",
    "          label_text = \"Red \" + score_light\n",
    "        else:\n",
    "          label_text = 'NO-LIGHT'  # This is not a traffic light\n",
    " \n",
    "    if color and label_text and accept_box(output[\"boxes\"], idx, 5.0) and score > 50:\n",
    "      cv2.rectangle(img_rgb, (box[\"x\"], box[\"y\"]), (box[\"x2\"], box[\"y2\"]), color, 2)\n",
    "      cv2.putText(img_rgb, label_text, (box[\"x\"], box[\"y\"]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    " \n",
    "  cv2.imwrite(output_file, cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR))\n",
    "  print(output_file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(box, coord_type):\n",
    "  \"\"\"\n",
    "  Get center of the bounding box.\n",
    "  \"\"\"\n",
    "  return (box[coord_type] + box[coord_type + \"2\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_object_detection(model, file_name, save_annotated=False, model_traffic_lights=None):\n",
    "  \"\"\"\n",
    "  Perform object detection on an image using the predefined neural network.\n",
    "  \"\"\"\n",
    "  # Store the image\n",
    "  img_bgr = cv2.imread(file_name)\n",
    "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "  input_tensor = tf.convert_to_tensor(img_rgb) # Input needs to be a tensor\n",
    "  input_tensor = input_tensor[tf.newaxis, ...]\n",
    " \n",
    "  # Run the model\n",
    "  output = model(input_tensor)\n",
    " \n",
    "  print(\"num_detections:\", output['num_detections'], int(output['num_detections']))\n",
    " \n",
    "  # Convert the tensors to a NumPy array\n",
    "  num_detections = int(output.pop('num_detections'))\n",
    "  output = {key: value[0, :num_detections].numpy()\n",
    "            for key, value in output.items()}\n",
    "  output['num_detections'] = num_detections\n",
    " \n",
    "  print('Detection classes:', output['detection_classes'])\n",
    "  print('Detection Boxes:', output['detection_boxes'])\n",
    " \n",
    "  # The detected classes need to be integers.\n",
    "  output['detection_classes'] = output['detection_classes'].astype(np.int64)\n",
    "  output['boxes'] = [\n",
    "    {\"y\": int(box[0] * img_rgb.shape[0]), \"x\": int(box[1] * img_rgb.shape[1]), \"y2\": int(box[2] * img_rgb.shape[0]),\n",
    "     \"x2\": int(box[3] * img_rgb.shape[1])} for box in output['detection_boxes']]\n",
    " \n",
    "  if save_annotated:\n",
    "    save_image_annotated(img_rgb, file_name, output, model_traffic_lights)\n",
    " \n",
    "  return img_rgb, output, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_shuffle(images, labels):\n",
    "  \"\"\"\n",
    "  Shuffle the images to add some randomness.\n",
    "  \"\"\"\n",
    "  indexes = np.random.permutation(len(images))\n",
    " \n",
    "  return [images[idx] for idx in indexes], [labels[idx] for idx in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_preprocess_inception(img_preprocessed):\n",
    "  \"\"\"\n",
    "  Reverse the preprocessing process.\n",
    "  \"\"\"\n",
    "  img = img_preprocessed + 1.0\n",
    "  img = img * 127.5\n",
    "  return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_object_detection_video(model, video_frame, model_traffic_lights=None):\n",
    "  \"\"\"\n",
    "  Perform object detection on a video using the predefined neural network.\n",
    "     \n",
    "  Returns the annotated video frame.\n",
    "  \"\"\"\n",
    "  # Store the image\n",
    "  img_rgb = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)\n",
    "  input_tensor = tf.convert_to_tensor(img_rgb) # Input needs to be a tensor\n",
    "  input_tensor = input_tensor[tf.newaxis, ...]\n",
    " \n",
    "  # Run the model\n",
    "  output = model(input_tensor)\n",
    " \n",
    "  # Convert the tensors to a NumPy array\n",
    "  num_detections = int(output.pop('num_detections'))\n",
    "  output = {key: value[0, :num_detections].numpy()\n",
    "            for key, value in output.items()}\n",
    "  output['num_detections'] = num_detections\n",
    " \n",
    "  # The detected classes need to be integers.\n",
    "  output['detection_classes'] = output['detection_classes'].astype(np.int64)\n",
    "  output['boxes'] = [\n",
    "    {\"y\": int(box[0] * img_rgb.shape[0]), \"x\": int(box[1] * img_rgb.shape[1]), \"y2\": int(box[2] * img_rgb.shape[0]),\n",
    "     \"x2\": int(box[3] * img_rgb.shape[1])} for box in output['detection_boxes']]\n",
    " \n",
    "  # For each bounding box that was detected  \n",
    "  for idx in range(len(output['boxes'])):\n",
    " \n",
    "    # Extract the type of the object that was detected\n",
    "    obj_class = output[\"detection_classes\"][idx]\n",
    "     \n",
    "    # How confident the object detection model is on the object's type\n",
    "    score = int(output[\"detection_scores\"][idx] * 100)\n",
    "         \n",
    "    # Extract the bounding box\n",
    "    box = output[\"boxes\"][idx]\n",
    " \n",
    "    color = None\n",
    "    label_text = \"\"\n",
    " \n",
    "    # if obj_class == LABEL_PERSON:\n",
    "      # color = (0, 255, 255)\n",
    "      # label_text = \"Person \" + str(score)\n",
    "    # if obj_class == LABEL_CAR:\n",
    "      # color = (255, 255, 0)\n",
    "      # label_text = \"Car \" + str(score)\n",
    "    # if obj_class == LABEL_BUS:\n",
    "      # color = (255, 255, 0)\n",
    "      # label_text = \"Bus \" + str(score)\n",
    "    # if obj_class == LABEL_TRUCK:\n",
    "      # color = (255, 255, 0)\n",
    "      # label_text = \"Truck \" + str(score)\n",
    "    if obj_class == LABEL_STOP_SIGN:\n",
    "      color = (128, 0, 0)\n",
    "      label_text = \"Stop Sign \" + str(score)\n",
    "    if obj_class == LABEL_TRAFFIC_LIGHT:\n",
    "      color = (255, 255, 255)\n",
    "      label_text = \"Traffic Light \" + str(score)\n",
    "             \n",
    "      if model_traffic_lights:\n",
    "       \n",
    "              # Annotate the image and save it\n",
    "        img_traffic_light = img_rgb[box[\"y\"]:box[\"y2\"], box[\"x\"]:box[\"x2\"]]\n",
    "        img_inception = cv2.resize(img_traffic_light, (299, 299))\n",
    "         \n",
    "        img_inception = np.array([preprocess_input(img_inception)])\n",
    " \n",
    "        prediction = model_traffic_lights.predict(img_inception)\n",
    "        label = np.argmax(prediction)\n",
    "        score_light = str(int(np.max(prediction) * 100))\n",
    "        if label == 0:\n",
    "          label_text = \"Green \" + score_light\n",
    "        elif label == 1:\n",
    "          label_text = \"Yellow \" + score_light\n",
    "        elif label == 2:\n",
    "          label_text = \"Red \" + score_light\n",
    "        else:\n",
    "          label_text = 'NO-LIGHT'  # This is not a traffic light\n",
    " \n",
    "    # Use the score variable to indicate how confident we are it is a traffic light (in % terms)\n",
    "    # On the actual video frame, we display the confidence that the light is either red, green,\n",
    "    # yellow, or not a valid traffic light.\n",
    "    if color and label_text and accept_box(output[\"boxes\"], idx, 5.0) and score > 20:\n",
    "      cv2.rectangle(img_rgb, (box[\"x\"], box[\"y\"]), (box[\"x2\"], box[\"y2\"]), color, 2)\n",
    "      cv2.putText(img_rgb, label_text, (box[\"x\"], box[\"y\"]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    " \n",
    "  output_frame = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "  return output_frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6628aac0378a00797c729f5a8d51d36d7263e4df468201e41567f7dd339f02e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
